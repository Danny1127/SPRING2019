<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Errors in numerical dynamic models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ivan Rudik" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Errors in numerical dynamic models
## <html>
<div style="float:left">

</div>
<hr color='#459DE0' size=5px width=1100px>
</html>
### Ivan Rudik
### AEM 7130

---


# Roadmap

1. Good general practice
2. The Euler equation error approach 
3. The `\(\chi\)`^2 approach
4. An approach to put a lower bound on errors

--

Things you will need:

1. VFI/FPI/TI code from the last section to use to check errors

---

# Good general practices

Here are some good general practices for solving a problem quick and accurately:

- Solve a coarser problem with fewer nodes and use the final solution as the initial guess for a problem with more nodes
  + In practice this tends to improve algorithm stability and speed
--

- Keep track if you are evaluating your value/policy function approximants **outside their defined domain**
  + This can be caused by actually transitioning outside the domain (bad),  
  or evaluating a state's quadrature node outside the domain (not quite as bad)
  + This can cause serious convergence problems because the routine will be extrapolating outside  
  the domain where it is well-defined, typically resulting in a bad approximation
--

- Determine which dimensions contain most of the curvature
  + Increase the degree of approximation along these dimensions, decrease on others
--

- Decent sized changes to your domain or number of collocation/quadrature nodes  
should not sigificantly change your solution
      
---

# Good general practices

- Keep the function being maximized as light as possible
  + It will be called thousands or millions of times so each line of code counts
  + **BE AWARE OF TYPE STABILITY IN FUNCTIONS CALLED FREQUENTLY**
  + This hasn't mattered too much in our simple examples, but it will in research paper quality problems
--

- Get your domain as tight as possible
  + This is one of the easiest yet biggest payoff things you can do
  + This will reduce the degree of approximation you require and can substantially reduce computing time
      - Cai et al. (2018) solve a 6-stated problem in minutes with a 4th degree approximation on a tight and time-varying grid
      - Rudik (2019) solves a difficult 5-stated problem in ~30 minutes on a tight and time-varying grid
      - Lemoine and Traeger (2014) takes a day on a wide and time-invariant grid
--
- Solve a determinsitic version of your model using another method that is known to be accurate
  + e.g. solve an open-loop version of the model using a KNITRO, SNOPT, CONOPT, etc
  + Compare open-loop to feedback solution
  
---

# Euler equation errors

The most common way to check model accuracy is by determining the error in the Euler equation as suggested by Judd (1992)

--

We know along any equilibrium trajectory the Euler equation must be satisfied exactly

--

Any errors in the Euler equation of a simulated set of trajectories must be due to numerical error

--

How do we compute the Euler error? Let us return to our growth example which had an Euler equation
`$$u'(c_t) = \beta u'(c_{t+1}) f'(k_{t+1})$$`

---

# Euler equation errors

`$$u'(c_t) = \beta u'(c_{t+1}) f'(k_{t+1})$$`

Notice that if we:

1. invert the marginal utility function on the left hand side
2. divide both sides by `\(c_t\)`, and then subtract the remaining 1

we arrive at an expression of the Euler equation that equals zero analytically, and is expressed in consumption units
`$$\frac{u'^{-1}(\beta \, u'(c_{t+1}) f'(k_{t+1}))}{c_t} - 1 = 0$$`

However when using our approximation policy function `\(C^{(p)}(k_{t})\)` we get
`$$EEE \equiv \frac{u'^{-1}(\beta \, u'(C^{(p)}(k_{t+1})) f'(k_{t+1}))}{C^{(p)}(k_{t})} - 1$$`
This is the Euler equation error (EEE) and will not necessarily be equal to zero

---

# Euler equation errors

Since EEE is in consumption units it has a nice economic interpretation: the relative optimization error due to following the approximated policy rule instead of the optimal policy rule

--

For example, if EEE = `\(10^{-3}\)`, then we are making a 1 dollar mistake  
for every 1000 dollars spent along our approximated policy trajectory

--

What is nice about the EEE, is that under certain conditions (Santos, 2000) the error in the policy rule is the same order of magnitude of the EEE, and the change in welfare is the square of the EEE

--

The required assumptions are
- Compactness and convexity of the feasible set
- Smoothness and strong concavity of the payoff function
- Interior solution

---

# EEE practice

Let's go back to our previous example and code and compute the EEE

Recall our problem was

--

`\begin{gather}
	\max_{\left\{c_t \right\}_{t=0}^\infty} \sum_{t=1}^\infty \beta^t u(c_t) \notag \\
	 \text{subject to:} \,\,\,\,\, k_{t+1} = f(k_t) - c_t \notag 
\end{gather}`
where both consumption and time `\(t+1\)` capital are positive, `\(k(0) = k_0\)`, `\(\alpha &gt; 0\)`, and `\(\beta \in (0,1)\)`

--

With parameters given by


```julia
using LinearAlgebra, Optim, Plots 
params = (alpha = 0.75, beta = 0.95, eta = 2,
                steady_state = (0.75*0.95)^(1/(1 - 0.75)), k_0 = (0.75*0.95)^(1/(1 - 0.75))*.75,
                capital_upper = (0.75*0.95)^(1/(1 - 0.75))*1.5, capital_lower = (0.75*0.95)^(1/(1 - 0.75))/2,
                num_points = 7, tolerance = 0.0001)
```

```
## (alpha = 0.75, beta = 0.95, eta = 2, steady_state = 0.25771486816406236, k_0 = 0.19328615112304676, capital_upper = 0.3865723022460935, capital_lower = 0.12885743408203118, num_points = 7, tolerance = 0.0001)
```

---

# Step 2: Select an initial vector of coefficients `\(b_0\)`


```julia
coefficients = .1*ones(params.num_points)
```

```
## 7-element Array{Float64,1}:
##  0.1
##  0.1
##  0.1
##  0.1
##  0.1
##  0.1
##  0.1
```

---

# Step 3: Select a convergence rule

Rule: maximum change in value on the grid &lt; 0.001%

---

# Step 4: Construct the grid and matrix of basis functions


```julia
function cheb_nodes(n)
    nodes = [cos.(pi * (2k - 1)/(2n)) for k = 1:n]
end;
grid = cheb_nodes(params.num_points) # [-1, 1] grid
```

```
## 7-element Array{Float64,1}:
##   0.9749279121818236   
##   0.7818314824680298   
##   0.4338837391175582   
##   6.123233995736766e-17
##  -0.43388373911755806  
##  -0.7818314824680295   
##  -0.9749279121818237
```

```julia
capital_grid = (1 .+ grid)*(params.capital_upper - params.capital_lower)/2 .+ params.capital_lower # actual capital grid
```

```
## 7-element Array{Float64,1}:
##  0.38334157734276386
##  0.3584596668794432 
##  0.31362401347666824
##  0.2577148681640623 
##  0.2018057228514564 
##  0.15697006944868153
##  0.13208815898536072
```

---

# Step 4: Construct the grid and basis matrix


```julia
# Chebyshev polynomial function
function cheb_polys(x, n)
    if n == 0
        return 1                  # T_0(x) = 1
    elseif n == 1
        return x                    # T_1(x) = x
    else
        cheb_recursion(x, n) =
            2x.*cheb_polys.(x, n - 1) .- cheb_polys.(x, n - 2)
        return cheb_recursion(x, n) # T_n(x) = 2xT_{n-1}(x) - T_{n-2}(x)
    end
end;
```

---

# Step 4a: Pre-invert your basis matrix

Hot tip: you will be using the exact same basis matrix in each loop iteration: just pre-invert it to save time


```julia
basis_matrix = [cheb_polys.(grid, n) for n = 0:params.num_points - 1];
basis_matrix = hcat(basis_matrix...)
```

```
## 7×7 Array{Float64,2}:
##  1.0   0.974928      0.900969  …   0.62349    0.433884      0.222521
##  1.0   0.781831      0.222521     -0.900969  -0.974928     -0.62349 
##  1.0   0.433884     -0.62349      -0.222521   0.781831      0.900969
##  1.0   6.12323e-17  -1.0           1.0        3.06162e-16  -1.0     
##  1.0  -0.433884     -0.62349      -0.222521  -0.781831      0.900969
##  1.0  -0.781831      0.222521  …  -0.900969   0.974928     -0.62349 
##  1.0  -0.974928      0.900969      0.62349   -0.433884      0.222521
```

```julia
basis_inverse = basis_matrix\I
```

```
## 7×7 Array{Float64,2}:
##  0.142857    0.142857    0.142857   …   0.142857    0.142857    0.142857 
##  0.278551    0.22338     0.123967      -0.123967   -0.22338    -0.278551 
##  0.25742     0.0635774  -0.17814       -0.17814     0.0635774   0.25742  
##  0.22338    -0.123967   -0.278551       0.278551    0.123967   -0.22338  
##  0.17814    -0.25742    -0.0635774     -0.0635774  -0.25742     0.17814  
##  0.123967   -0.278551    0.22338    …  -0.22338     0.278551   -0.123967 
##  0.0635774  -0.17814     0.25742        0.25742    -0.17814     0.0635774
```

---

# Step 5: Loop

Construct a function that loops over the grid points and solves the Bellman given `\(\Gamma(x;b^{(p)})\)`


```julia
function loop_grid(params, basis_inverse, basis_matrix, grid, capital_grid, coefficients)

    max_value = -.0*ones(params.num_points);
    scale_capital(capital) = 2*(capital - params.capital_lower)/(params.capital_upper - params.capital_lower) - 1

    # Compute next period's consumption from the Euler equation
    for (iteration, capital) in enumerate(capital_grid)

        function bellman(consumption)
            capital_next = capital^params.alpha - consumption
            capital_next_scaled = scale_capital(capital_next)
            cont_value = coefficients' * [cheb_polys.(capital_next_scaled, n) for n = 0:params.num_points - 1]
            value_out = (consumption)^(1-params.eta)/(1-params.eta) + params.beta*cont_value
            return -value_out
        end;

        results = optimize(bellman, 0.00*capital^params.alpha, 0.99*capital^params.alpha)

        # Compute new value
        max_value[iteration] = -Optim.minimum(results)
    end

    return max_value
end
```

```
## loop_grid (generic function with 1 method)
```

---

# Step 5: Loop


```julia
function solve_vfi(params, basis_inverse, basis_matrix, grid, capital_grid, coefficients)
    iteration = 1
    error = 1e10;
    max_value = -.0*ones(params.num_points);
    value_prev = .1*ones(params.num_points);
    coefficients_store = Vector{Vector}(undef, 1)
    coefficients_store[1] = coefficients
    while error &gt; params.tolerance
        max_value = loop_grid(params, basis_inverse, basis_matrix, grid, capital_grid, coefficients)
        coefficients = basis_inverse*max_value # \Psi \ y
        error = maximum(abs.((max_value - value_prev)./(value_prev)))
        value_prev = deepcopy(max_value)
        if mod(iteration, 5) == 0
            println("Maximum Error of $(error) on iteration $(iteration).")
            append!(coefficients_store, [coefficients])
        end
        iteration += 1
    end
    return coefficients, max_value, coefficients_store
end
```

```
## solve_vfi (generic function with 1 method)
```

---

# Step 5: Loop


```julia
solution_coeffs, max_value, intermediate_coefficients = 
    solve_vfi(params, basis_inverse, basis_matrix, grid, capital_grid, coefficients)
```

```
## Maximum Error of 0.2737801075761883 on iteration 5.
## Maximum Error of 10.181402299701638 on iteration 10.
## Maximum Error of 0.24010896371104573 on iteration 15.
## Maximum Error of 0.08969271235173859 on iteration 20.
## Maximum Error of 0.04951271957737897 on iteration 25.
## Maximum Error of 0.031335744632027096 on iteration 30.
## Maximum Error of 0.02124544109347175 on iteration 35.
## Maximum Error of 0.015000169652694507 on iteration 40.
## Maximum Error of 0.010869867868583969 on iteration 45.
## Maximum Error of 0.008016830712855458 on iteration 50.
## Maximum Error of 0.0059861989167673026 on iteration 55.
## Maximum Error of 0.004509877146521781 on iteration 60.
## Maximum Error of 0.0034198806821233935 on iteration 65.
## Maximum Error of 0.0026059187406616727 on iteration 70.
## Maximum Error of 0.001992913806429802 on iteration 75.
## Maximum Error of 0.001528298601676595 on iteration 80.
## Maximum Error of 0.0011744475125273625 on iteration 85.
## Maximum Error of 0.0009039617707066463 on iteration 90.
## Maximum Error of 0.0006966193080105799 on iteration 95.
## Maximum Error of 0.0005373371780118751 on iteration 100.
## Maximum Error of 0.00041477290357942395 on iteration 105.
## Maximum Error of 0.0003203422150018359 on iteration 110.
## Maximum Error of 0.0002475159621255567 on iteration 115.
## Maximum Error of 0.00019130889470004935 on iteration 120.
## Maximum Error of 0.00014790315766932002 on iteration 125.
## Maximum Error of 0.00011436811226017789 on iteration 130.
```

```
## ([-194.522, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-182.756, -184.216, -187.237, -191.985, -198.441, -205.762, -211.259], Array{T,1} where T[[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [46.5073, 7.85518, -1.59278, 0.349952, -0.078987, 0.0174254, -0.00346848], [-7.65552, 12.297, -2.38151, 0.525487, -0.124571, 0.0307752, -0.00663628], [-50.0685, 13.6042, -2.58269, 0.562034, -0.132302, 0.0337529, -0.00764398], [-82.8567, 13.9846, -2.63927, 0.571235, -0.133559, 0.0342889, -0.00789938], [-108.206, 14.0957, -2.65616, 0.574042, -0.133889, 0.034416, -0.00796877], [-127.813, 14.1281, -2.66118, 0.574898, -0.133991, 0.0344524, -0.00798883], [-142.982, 14.1375, -2.66264, 0.575151, -0.134022, 0.0344631, -0.00799468], [-154.718, 14.1402, -2.66307, 0.575226, -0.134031, 0.0344662, -0.00799639], [-163.8, 14.141, -2.66319, 0.575247, -0.134033, 0.0344671, -0.00799688]  …  [-190.871, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-191.774, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-192.472, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-193.013, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-193.431, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-193.755, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-194.006, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-194.2, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-194.35, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708], [-194.466, 14.1413, -2.66324, 0.575256, -0.134034, 0.0344675, -0.00799708]])
```

---

# Step 6: Simulate


```julia
scale_capital(capital) = 2*(capital - params.capital_lower)/(params.capital_upper - params.capital_lower) - 1;

function simulate_model(params, solution_coeffs, time_horizon = 100)
    capital_store = zeros(time_horizon + 1)
    consumption_store = zeros(time_horizon)
    capital_store[1] = params.k_0
    
    for t = 1:time_horizon
        capital = capital_store[t]
        function bellman(consumption)
            capital_next = capital^params.alpha - consumption
            capital_next_scaled = scale_capital(capital_next)
            cont_value = solution_coeffs' * [cheb_polys.(capital_next_scaled, n) for n = 0:params.num_points - 1]
            value_out = (consumption)^(1-params.eta)/(1-params.eta) + params.beta*cont_value
            return -value_out
        end;

        results = optimize(bellman, 0.0, capital^params.alpha)

        # Compute new value
        consumption_store[t] = Optim.minimizer(results)
        capital_store[t+1] = capital^params.alpha - consumption_store[t]
    end

    return consumption_store, capital_store
end;
```

---

# Step 6: Simulate


```julia
time_horizon = 100;
consumption, capital = simulate_model(params, solution_coeffs, time_horizon);
```

---

# Step 7: Compute Euler errors


```julia
# Compute Euler equation error at each time period
# Put into log10 terms of the absolute value of the error. More negative numbers are better.
# -4 means error is 10^-4, -3 means the error is 10^-3, etc.
euler_error = log10.(abs.(
  (params.beta*consumption[2:end].^(-params.eta).*
     params.alpha.*capital[2:end-1].^(params.alpha-1)).^(-1/params.eta)./
    consumption[1:end-1] .- 1))
```

```
## 99-element Array{Float64,1}:
##  -3.4339687375801833
##  -3.5368998209008016
##  -3.774788682401588 
##  -4.3936814351078395
##  -4.143349140780453 
##  -3.7897334413964567
##  -3.6363469105814503
##  -3.5504457433480243
##  -3.498432549918634 
##  -3.465488770011638 
##   ⋮                 
##  -3.4064032286654804
##  -3.406349138144159 
##  -3.4063912100192173
##  -3.4064243912288816
##  -3.4063821777343217
##  -3.406349063642609 
##  -3.4064078833882467
##  -3.406370140600381 
##  -3.4064078838186926
```

---

# Euler error along eq trajectory

&lt;img src="7_error_analysis_files/figure-html/unnamed-chunk-12-J1.png" width="800" /&gt;

---

# Euler error over the entire domain


```julia
num_points = 1000
```

```
## 1000
```

```julia
euler_error = zeros(num_points)
```

```
## 1000-element Array{Float64,1}:
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  ⋮  
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
```

```julia
capital_levels = range(params.capital_lower, params.capital_upper, length = num_points);

function simulate_model_ee(params, solution_coeffs, capital_levels)
    consumption_store = similar(capital_levels)
    for (iteration, capital) in enumerate(capital_levels)
        function bellman(consumption)
            capital_next = capital^params.alpha - consumption
            capital_next_scaled = scale_capital(capital_next)
            cont_value = solution_coeffs' * [cheb_polys.(capital_next_scaled, n) for n = 0:params.num_points - 1]
            value_out = (consumption)^(1-params.eta)/(1-params.eta) + params.beta*cont_value
            return -value_out
        end;
        results = optimize(bellman, 0.0, capital^params.alpha)
        consumption_store[iteration] = Optim.minimizer(results)
    end
    return consumption_store
end;
num_points = 1000;
euler_error = zeros(num_points);
capital_levels = range(params.capital_lower, params.capital_upper, length = num_points);
consumption = simulate_model_ee(params, solution_coeffs, capital_levels);
euler_error = log10.(abs.((params.beta*consumption[2:end].^(-params.eta).*params.alpha.*capital_levels[2:end].^(params.alpha-1)).^(-1/params.eta)./consumption[1:end-1] .- 1))
```

```
## 999-element Array{Float64,1}:
##  -1.0878243214493006
##  -1.0890341682453437
##  -1.0902451982329742
##  -1.0914568866124807
##  -1.0926705867539999
##  -1.0938850380258736
##  -1.0951010019254606
##  -1.0963184423470183
##  -1.0975368738878404
##  -1.0987566836760632
##   ⋮                 
##  -1.2866831167174078
##  -1.2859440305976844
##  -1.2852063165308023
##  -1.2844713285769311
##  -1.2837367452685513
##  -1.283004330075062 
##  -1.2822736723912276
##  -1.2815445066223228
##  -1.280816956590286
```

---

# Euler error over the entire domain

&lt;img src="7_error_analysis_files/figure-html/unnamed-chunk-14-J1.png" width="800" /&gt;

---

# General dynamic programming errors

Santos and Vigo-Aguiar (1998) show us how to study the error of a DP model

--

Let:
- `\(\beta\)` be the discount factor
- `\(||\cdot||\)` be the sup norm
- `\(h\)` be the maximum distance between any two grid points that form a simplex in our approximation grid
- `\(\gamma = ||D^2 V||\)`, the maximal element of the Hessian of the true value function `\(V\)`
- `\(V^h\)` be our value function approximant on our grid with maximum distance `\(h\)`

---

# General dynamic programming errors

Assume the following points are true:

1. The state space is an open set, and the feasible set is convex
--

2. The flow payoff function is bounded, twice-continuously differentiable, and has bounded first and second order derivatives
--

3. There exists some constant `\(\eta &gt; 0\)` such that `\(v(k,k',z_t) + (\eta/2)||k'||^2\)` is concave as a function on `\((k,k')\)`
--

4. For each `\((k_0,z_t)\)` there exists an optimal solution to the Bellman equation such that the equilibrium path is on the interior of the feasible set
--

5. The transition function for exogenous processes is twice-continuously differentiable, and the derivative of the transition function with respect to the exogenous state, and the mixed derivative with respect to the exogenous state and noise term are bounded, and jointly continuous over the state space
--

6. There also exist nonnegative constants `\(C \geq 0\)`, `\(0 \leq \rho &lt; \beta^{-1/2}\)` such that the sup norms of the first and second order partial derivatives of the exogenous state `\(z_t\)` with respect to `\(z_0\)` are less than `\(C\rho^t\)` for all `\(t\)`
--

In general these are standard assumptions satisfied by most problems

---

# General dynamic programming errors

This gives us the following theorem:  
*Let `\(V\)` be the fixed point of the Bellman equation, and let `\(V^h\)` be the fixed point of the Bellman equation on our collocation grid. Under assumptions 1-6 it must be that*
`$$||V-V^h|| \leq \frac{\gamma}{2(1-\beta)}h^2$$`

--

The maximal error in our value function can thus be bounded above and is decreasing quadratically in the resolution of our grid

--

This also leads to a corollary,  
*Let `\(g\)` be the policy function corresponding to `\(V\)` and let `\(g^h\)` be the policy function corresponding to `\(V^h\)`. At all collocation grid points we have that*
`$$||g - g^h|| \leq \sqrt{\frac{2\gamma}{\eta(1-\beta)}}\,h$$`

---

# General dynamic programming errors

A few points:

- `\(\gamma\)` is defined as the sup matrix norm of the Hessian of the *true* value function which is generally unknown

--

However we can put an upper bound on this using inequalities (3.2)-(3.5) in Santos and Vigo-Aguiar (1998)

--

- The linear convergence of the policy function can be extended to any point in the domain, not just the collocation nodes


---

# Alternatives

Some problems do not permit an Euler equation

--

You can always error check other equilibrium conditions (Bellman, etc) by finding ways to put them into consumption units

-- 

They will not have the same policy rule and welfare error bounds though

---

# `\(\chi^2\)` error tests

den Haan and Marcet (1994) provide an alternative way to test the approximation quality of a model via hypothesis testing

--

Suppose that we have some set of variables `\(y_t\)` that completely characterize our state at time `\(t\)` and can be exogenous or endogenous

--

Typically we will have a system of equations such that the following must be satisfied at a solution
`$$f(y_t) = E[\phi(y_{t+1},y_{t+2},...)|\Omega_t]$$`

here `\(f:\mathbb{R}^n\rightarrow\mathbb{R}^m\)` and `\(\phi:\mathbb{R}^n\times\mathbb{R}^\infty\rightarrow\mathbb{R}^m\)` are known functions, and `\(\Omega_t\)` is the current information set

--

This is very general and can accommodate most types of models

--

If the above conditions are satisfied then we must have that the residual
`$$u_{t+1} = \phi(y_{t+1},y_{t+2},...)-f(y_t)$$`
satisfies the following
`$$E[u_{t+1} \otimes h(x_t)] = 0$$`
for any `\(k\)`-dimensional vector `\(x_t\)` in `\(\Omega_t\)`, and any function `\(h:\mathbb{R}^K\rightarrow\mathbb{R}^q\)`

---

# `\(\chi^2\)` error tests

`$$E[u_{t+1} \otimes h(x_t)] = 0$$`

What we want to do is to test whether this equation is true along a given simulated path

--

This is basically a moment condition

--

If we simulate a series of arbitrary length from our model with some solution method and get some `\(\{y^j_t\}_{t=1:T}\)`, we can find `\(\{u^j_{t+1},x^j_t\}_{t=1:T}\)` and compute the sample analog of the Kronecker product
`$$B^j_T = \frac{1}{T}\sum_{t=1}^T u^j_{t+1} \otimes h(x_t^j)$$`

--

Clearly `\(B^j_T\)` converges to zero if we are using the exact model

---

# `\(\chi^2\)` error tests

We can also make it arbitrarily small by picking `\(h\)` to have small values, so we need to be careful

--

We avoid this problem by constructing a test statistic for `\(B^j_T\)` under the hypothesis our model is accurate

--

This test statistic is
`$$T(B^j_T)'(A^j_T)^{-1}B^j_T$$`
where `\(A^j_T\)` is a consistent estimate of
`$$\sum_{t=-\infty}^{\infty} E[(u_{t+1} \otimes h(x_t))(u_{t+1} \otimes h(x_t))']$$`

--

If `\(\phi\)` only depends on `\(y_{t+1}\)` then we have the simplest case
`$$A^j_T = \frac{1}{T}\sum_{t=1}^T u_{t+1}^2 h(x_t) h(x_t)'$$`

---

# `\(\chi^2\)` error tests

`$$A^j_T = \frac{1}{T}\sum_{t=1}^T u_{t+1}^2 h(x_t) h(x_t)'$$`

This test statistic will converge to a `\(\chi^2\)` distribution with `\(qm\)` degrees of freedom with a null hypothesis that `\(E[u_{t+1} \otimes h(x_t)] = 0\)`

--

If the value of the test statistic is in the upper or lower critical regions, we can reject that we have a good approximation

--

As `\(T\rightarrow \infty\)` we will eventually reject the null. Why?

--

No approximating model is perfect and with enough data, errors continually accumulate and the test statistic will learn its not the true model

--

To control for this they suggest repeating the rest for many simulations and then checking how many of the test statistics are in the upper and lower 5\% of the distribution

--

If the approximation is good, both should be about 5\%: this is a good test for checking how errors accumulate over time
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "bash",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Projection methods</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ivan Rudik" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Projection methods
## <html>
<div style="float:left">

</div>
<hr color='#459DE0' size=5px width=1100px>
</html>
### Ivan Rudik
### AEM 7130

---


# Roadmap

1. What is projection
2. How we approximate functions

---

# Our basic dynamic model

An arbitrary infinite horizon problem can be represented using a Bellman equation
`$$V(S_t) = \max_{q_t} u(q_t) + \beta\,V(S_{t+1}(q_t))$$`

--

We can start from any arbitrary initial state vector and simulate the optimal policy paths in deterministic or stochastic settings

--

We know that this problem has a fixed point (solution) from previous lectures, but *how* do we arrive at our fixed point, `\(V^*\)`?

--

One way to do this is via **projection**

--

Or perturbation, or discretization, or some analytic approaches for specific models

--

We'll focus on value functions here, but we can solve dynamic models by approximating policy functions as well

---

# Projection methods

**Main idea:** build some function `\(\hat{V}\)` indexed by coefficients that approximately solves the Bellman

--

What do I mean by approximately?

--

The coefficients of `\(\hat{V}\)` are selected to minimize some residual function  
that tells us how far away our solution is from solving the Bellman 

--

Rearrange the Bellman equation and define a new functional `\(H\)` that maps the problem into a more general framework
`$$H(V) = V(S_t) - \max_{q_t} u(q_t) + \beta\,V(S_{t+1}(q_t))$$`

--

We can find some function `\(V\)` that solves `\(H(V) = 0\)`
 

---

# Projection methods

How do we do this?

--

We solve this by specifying some linear combination of **basis functions** `\(\Psi_i(\mathbf{S})\)`
`$$V^j(\mathbf{S}|\theta) = \sum_{i=0}^j \theta_i \Psi_i(\mathbf{S})$$`
with coefficients `\(\theta_0,...,\theta_j\)`

--

We then define a residual
`$$R(\mathbf{S}|\theta) = H(V^j(\mathbf{S}|\theta))$$`
and select the coefficient values to minimize the residual, given some measure of distance

--

This step of selecting the coefficients is called **projecting** `\(H\)` against our basis

--

We still have some choices to make: What basis do we select? How do we project (select the coefficients)?

---

# Projection methods

At first this may be a pretty confusing concept

--

Simple example to get intuition?

--

Ordinary least squares linear regression

--

We can think of the problem as searching for some unknown conditional expectation `\(E[Y|X]\)`,  
given outcome variable `\(Y\)` and regressors `\(X\)`

--

We don't know the true functional form, but we can approximate it using the first two monomials on `\(X\)`: `\(1\)` and `\(X\)`
`$$E[Y|X] \approx \theta_0 + \theta_1 X$$`

--

These are the first two elements of the *monomial basis*

--

One residual function is then: `\(R(Y,X|\theta_0,\theta_1) = abs(Y-\theta_0-\theta_1 X)\)`

--

In econometrics we often minimize the square of this residual, which is just OLS

--

So OLS is within the class of projection methods

--

In OLS we use observed data, but in theory we use the operator `\(H(V)\)`

---

# Projection classes are defined by metrics

Projection methods are separated into several broad classes by the type of residual we're trying to shrink to zero

--

We need to select some metric function `\(\rho\)`, that determines how we project

--

`\(\rho\)` tells us how close our residual function is to zero over the domain of our state space

---

# Example residuals given different projections

Example: The figure shows two different residuals on some capital domain of `\([0,\bar{k}]\)`

&lt;div align="center"&gt;
  &lt;img src="figures/residuals.png" height=350&gt;
&lt;/div&gt;


The residual based on the coefficient vector `\(\theta_1\)` is large for small values of capital but near-zero everywhere else

--

The residual based on `\(\theta_2\)` has medium values just about everywhere

--

Which is closer to zero over the interval? It will depend on our selection of `\(\rho\)`

---

# We want to use weighted residuals

We move from the plain residual to `\(\rho\)` because we want to set a *weighted residual* equal to zero (more general)

--

Suppose we have some weight functions `\(\phi_i:\Omega \rightarrow \mathbb{R}\)` that map from our state space to the real line

--

The one-dimensional metric is defined as
`\begin{equation*}
	\rho(R\cdot|\theta,0) =
	\begin{cases}
		0 &amp; \text{if } \int_\Omega\phi_i(\mathbf{S})R(\cdot|\theta)d\mathbf{S} = 0, i=1,...,j+1\\
		1 &amp;\text{otherwise}
	\end{cases}
\end{equation*}`

--

Where we want to solve for `\(\theta = \text{argmin} \, \rho(R(\cdot|\theta),0)\)`

--

We can then change our problem to simply solving a system of integrals ensuring the metric is zero
`$$\int_\Omega\phi_i(\mathbf{S})R(\cdot|\theta)d\mathbf{S} = 0, i=1,...,j+1.$$` 

--

We can solve this using standard rootfinding techniques

--

**Big remaining question:** how do we choose our `\(j+1\)` weight functions?

--

First lets begin with a simple example before moving into the most commonly used weight functions

---

# Least squares projection

Suppose we selected the weight function to be
`$$\phi_i(\mathbf{S}) = \frac{\partial R(\mathbf{S}|\theta)}{\partial \theta_{i-1}}$$`

--

Then we would be performing least squares! Why?

--

Recall the objective of least squares is
`$$\min_{\mathbf{\theta}} \int R^2(\cdot|\theta)d\mathbf{S}$$`
--

The FOC for a minimum is
`$$\int \frac{\partial R(\mathbf{S}|\theta)}{\partial \theta_{i-1}}R(\cdot|\theta)d\mathbf{S} = 0, i = 1,...,j+1$$`

--

So the first order condition sets the weighted average residual to zero  
where the weights are determined by the partial derivatives of the residual

--

The partial derivative weight function yields a metric function that solves the least squares problem!


---

# Collocation

The most commonly used weight function gives us a methodology called **collocation**

--

Here our weight function is
`$$\phi_i(\mathbf{S}) = \delta(\mathbf{S}-\mathbf{S}_i)$$`
Where `\(\delta\)` is the *Dirac delta function* and `\(\mathbf{S}_i\)` are the j+1 *collocation points* or *collocation nodes* selected by the researcher

--

The Dirac delta function is zero at all `\(\mathbf{S}\)` except at `\(\mathbf{S} = \mathbf{S}_i\)`

--

What does this weight function mean?

--

Before we even select our coefficients, this means that the residual can only be non-zero at a finite set of points `\(\mathbf{S_i}\)`

--

So the solution to our problem must set the residual to zero at these collocation points

--

Since we have a finite set of points we do not need to solve difficult integrals but only a system of equations
`$$R(\mathbf{S}_i|\theta) = 0, i=1,...,j+1$$`

---

# Rough idea of how we proceed

Collocation methods aim to approximate the value function `\(V(S_t)\)` with  
some linear combination of known, and usually orthogonal ($\int f(x)g(x)w(x)=0$), basis functions

--

One example of a possible class of basis functions is the monomial basis: `\(x, x^2, x^3,...\)`

--

But how do we implement collocation?

--

Recall we solve for coefficients `\(\theta\)` by setting the residual to be zero at all of our collocation points

--

But the residual is a function of `\(V\)`, and thus will be a function of `\(\theta\)`, a loop seemingly impossible to escape

--

We can solve this problem by *iterating* on the problem,  
continually setting the residuals equal to zero, recovering new `\(\theta\)`s, and repeating

---

# Rough idea of how we proceed

In any given iteration, we:

--

1. Begin with a vector of coefficients on the basis functions (e.g. a random guess)

--

2. Use a linear combination of the basis functions as an approximation to the value function on the right hand side of the Bellman

--

3. Solve the Bellman with the approximated value function in it, at our set of collocation points

--

4. Recover a set maximized continuation values at these points in our state space conditional on the previous value function approximant we used

--

5. Use these new maximized values to obtain updated coefficients solving the system of linear equations, and repeat the process until we have "converged"

--

By the contraction mapping theorem this is guaranteed to converge from any arbitrary initial set of coefficients! (conditional on satisfying the assumptions and there being no numerical error..)

---

# Still have some questions

This still leaves several questions unanswered

--

- Where in the state space do we place the collocation points?

--

- What basis functions do we use?

--

Do your answers to the previous two questions really matter?

--

**Yes they are crucial**

--

Schemes exist to generate high quality approximations, and to obtain the approximation at low computational cost

--

Once we have recovered an adequate approximation to the value function, we have effectively solved the entire problem!

--

We know how the policymaker's expected discounted stream of future payoffs changes as we move through the state space

--

We can solve the Bellman at some initial state and know that solving the Bellman will yield us optimal policies

--

Therefore we can simulate anything we want and recover optimal policy functions  
given many different sets of initial conditions or realizations of random variables

---

# Interpolation

What points in our state space do we select to be collocation points?

--

We often have continuous states in economics (capital, temperature, oil reserves, technology shocks, etc.),  
so we must have some way to reduce the infinity of points in our state space into something more manageable

--

We do so by selecting a specific finite number of points in our state space 
and use them to construct a *collocation grid* that spans the domain of our problem

--

Using our knowledge of how the value function behaves at the limited set of points on our grid,  
we can interpolate our value function approximant at all points off the grid points,  
but *within* the domain of our grid

--

**Note:** The value function approximant is not very good outside the grid's domain since that would mean  
extrapolating beyond whatever information we have gained from analyzing our value function on the grid

---

# Basis functions

Let `\(V\)` be the value function we wish to approximate with some `\(\hat{V}\)`

--

`\(\hat{V}\)` is constructed as a linear combination of `\(n\)` linearly independent (i.e. orthogonal) basis function
`$$\hat{V}(x)= \sum_{j=1}^n c_j \psi_j(x)$$`

--

Each `\(\psi_j(x)\)` is a basis function, and the coefficients `\(c_j\)` determine how they are combined  
at some point `\(\bar{x}\)` to yield our approximation `\(\hat{V}(\bar{x})\)` to `\(V(\bar{x})\)`

--

The number of basis functions we select, `\(n\)`, is the degree of interpolation

--

In order to recover `\(n\)` coefficients, we need at least `\(n\)` equations that must be satisfied at a solution to the problem

--

If we have precisely `\(n\)` equations, we are just solving a simple system of linear equations:  
we have a perfectly identified system and are solving a collocation problem

--

This is what happens we select our number of grid points in the state space  
to be equal to the number of coefficients (which induces a Dirac delta weighting function)

---

# Basis functions

In this case, we can:

--

1. Solve the Bellman at the `\(n\)` grid points

--

2. Recover the maximized values

--

3. Solve a system of equations, *linear in `\(c_j\)`* that equates the value function approximant at the grid points to the recovered maximized values
`$$\Psi \mathbf{c} = \mathbf{y}$$`
where `\(\Psi\)` is the matrix of polynomials, `\(c\)` is a vector of coefficients, and `\(y\)` is a vector of the maximized values of the Bellman

--

We can recover `\(c\)` by left dividing by `\(\Psi\)` which yields
`$$\mathbf{c} = \Psi^{-1}\mathbf{y}$$`

--

If we have more equations, or grid points, than coefficients,  
then we can just use OLS to solve for the coefficients by minimizing the sum of squared errors

--

There are many ways to do projection but we will work with collocation since it is the most common

---

# (Pseudo-)spectral methods

Spectral methods apply all of our basis functions to the entire domain of our grid: they are global

--

When using spectral methods we virtually always use polynomials

--

**Why?**

--

The Stone-Weierstrass Theorem which states (for one dimension)

*Suppose `\(f\)` is a continuous real-valued function defined on the interval `\([a,b]\)`.  
For every `\(\epsilon &gt; 0, \,\, \exists\)` a polynomial `\(p(x)\)` such that for all `\(x \in [a,b]\)` we have `\(||f(x) - p(x)||_{sup} \leq \epsilon\)`*

--

What does the SW theorem say in words?

--

For any continuous function `\(f(x)\)`, we can approximate it arbitrarily well with some polynomial `\(p(x)\)`, as long as `\(f(x)\)` is continuous

--

This means the function can even have kinks

--

Unfortunately we do not have infinite computational power so solving kinked problems with spectral methods is not advised

--

Note that the SW theorem *does not* say what kind of polynomial can approximate `\(f\)` arbitrarily well,  
just that some polynomial exists

---

# Basis choice

What would be your first choice of basis?

--

Logical choice: the monomial basis: `\(1, x, x^2,...\)`

--

It is simple, and SW tells us that we can uniformly approximate any continuous function on a closed interval using them

--

## Practice

code up a function `project_monomial(f, n, lb, ub)` that takes in some function `f`,  
degree of approximation `n`, lower bound `lb` and upper bound `ub`,  
and constructs a monomial approximation on an evenly spaced grid via collocation

--

We will be plotting stuff, see `http://docs.juliaplots.org/latest/examples/gr/` for example code using the `GR` backend

--

Initialize with:

```julia
using Plots, LinearAlgebra
gr()
```

```
## Plots.GRBackend()
```

---

# The monomial basis

Let's approximate `sin(x)`


```julia
using Plots
gr();
f(x) = sin.(x);

Plots.plot(f, 0, 2pi, line = 4, grid = false, legend = false, size = (500, 250))
```

&lt;img src="5_proj_files/figure-html/unnamed-chunk-2-J1.png" width="667" /&gt;

---

# Approximating sin(x)


```julia

function project_monomial(f, n, lb, ub)
    # solves Ψc = y → c = Ψ\y
    # Ψ = matrix of monomial basis functions evaluted on the grid

    coll_points = range(lb, ub, length = n)                      # collocation points
    y_values = f(coll_points)                                    # function values on the grid
    basis_functions = [coll_points.^degree for degree = 0:n-1]   # vector of basis functions
    basis_matrix = hcat(basis_functions...)                      # basis matrix

    coefficients = basis_matrix\y_values                         # c = Ψ\y

    return coefficients

end;

coefficients_5 = project_monomial(f, 5, 0, 2pi);
coefficients_10 = project_monomial(f, 10, 0, 2pi)
```

```
## 10-element Array{Float64,1}:
##   0.0                  
##   0.9990725797458863   
##   0.004015857153649684 
##  -0.1738437387373486   
##   0.007075663351639969 
##   0.004040763230876247 
##   0.0016747985983553285
##  -0.0006194667844101428
##   6.485272688203222e-5 
##  -2.293696012495368e-6
```

---

# Approximating sin(x)

Now we need to construct a function `f_approx(coefficients, plot_points)` that takes in the `coefficients` vector,  
and an arbitrary vector of points to evaluate the approximant at (for plotting)

--


```julia
function f_approx(coefficients, points)
    n = length(coefficients) - 1
    basis_functions = [coefficients[degree + 1] * points.^degree for degree = 0:n] # evaluate basis functions
    basis_matrix = hcat(basis_functions...)                                        # transform into matrix
    function_values = sum(basis_matrix, dims = 2)                                  # sum up into function value
    return function_values
end;
```

--


```julia
plot_points = 0:.01:2pi;
f_values_5 = f_approx(coefficients_5, plot_points);
f_values_10 = f_approx(coefficients_10, plot_points)
```

```
## 629×1 Array{Float64,2}:
##   0.0                  
##   0.009990953610597868 
##   0.019981668333012258 
##   0.029971103713554246 
##   0.03995822109623334  
##   0.04994198367415802  
##   0.05992135654204973  
##   0.06989530674983996  
##   0.07986280335732032  
##   0.0898228174898161   
##   ⋮                    
##  -0.08303623493742407  
##  -0.0730710101597829   
##  -0.06309900382682798  
##  -0.05312124622683001  
##  -0.04313876967565733  
##  -0.03315260846174084  
##  -0.023163798791941304 
##  -0.01317337873884128  
##  -0.0031823881890886696
```

---

# Plot

&lt;img src="5_proj_files/figure-html/unnamed-chunk-6-J1.png" width="800" /&gt;

---

# Plot

&lt;img src="5_proj_files/figure-html/unnamed-chunk-7-J1.png" width="800" /&gt;

---

# Plot

&lt;img src="5_proj_files/figure-html/unnamed-chunk-8-J1.png" width="800" /&gt;

---

# Cool!

We just wrote some code that exploits Stone-Weierstrauss and allows us to (potentially) approximate  
any continuous function arbitrarily well as `n` goes to infinity

--

To approximate any function we'd need to feed in some basis function `g(x, n)`  
as opposed to hard-coding it like I did in the previous slides

--

Turns out we never use the monomial basis though 

--

&lt;div align="center"&gt;
  &lt;img src="figures/shrug.jpeg" height=250&gt;
&lt;/div&gt;

--

Why?

--

Try approximating **Runge's function**: `f(x) = 1/(1 + 25x^2)`

---

# Runge's function


```julia
runge(x) = 1 ./ (1 .+ 25x.^2);
coefficients_5 = project_monomial(runge, 5, -1, 1);
coefficients_10 = project_monomial(runge, 10, -1, 1);
plot_points = -1:.01:1;
runge_values_5 = f_approx(coefficients_5, plot_points);
runge_values_10 = f_approx(coefficients_10, plot_points);
```

---

# Runge's function

&lt;img src="5_proj_files/figure-html/unnamed-chunk-10-J1.png" width="800" /&gt;

---

# Runge's function

&lt;img src="5_proj_files/figure-html/unnamed-chunk-11-J1.png" width="800" /&gt;

---

# Runge's function

&lt;img src="5_proj_files/figure-html/unnamed-chunk-12-J1.png" width="800" /&gt;

---

# Maybe we can just crank up n?

&lt;img src="5_proj_files/figure-html/unnamed-chunk-13-J1.png" width="800" /&gt;

---

# Maybe we can just crank up n?

&lt;img src="5_proj_files/figure-html/unnamed-chunk-14-J1.png" width="800" /&gt;

---

# Monomials are not good

What's the deal?

--

The matrix of polynomials, `\(\Phi\)`, is often ill-conditioned, especially as the degree of the polynomials increases

--

The first 6 monomials can induce a condition number of `\(10^{10}\)`, a substantial loss of precision

--

Second, they can vary dramatically in size, which leads to scaling/truncation errors

--


```julia
runge(x) = 1 ./ (1 .+ 25x.^2);
coefficients_10 = project_monomial(runge, 10, -1, 1);
points = rand(10);
n = length(coefficients_10) - 1;
basis_functions = [coefficients_10[degree + 1] * points.^degree for degree = 0:n];
basis_matrix = hcat(basis_functions...);
println("Condition number: $(cond(basis_matrix))")
```

```
## Condition number: 1.495541815335492e20
```

--

Example: `\(x^11\)` goes from `\(1e-4\)` to about 90 when moving x from 0.5 to 1.5

--

Ideally we want an **orthogonal basis:** when we add another element of the basis,  
it has sufficiently different behavior than the elements before it so it can capture  
features of the unknown function that the previous elements couldn't

---

# The Chebyshev basis

Most frequently used is the **Chebyshev basis**

--

It has nice approximation properties:

1. They are easy to compute

--

2. They are orthogonal

--

3. They are bounded between `\([-1,1]\)`

--

Chebyshev polynomials are often selected because they minimize the oscillations that occur when approximating functions like Runge's function

--

The Chebyshev polynomial closely approximates the *minimax polynomial*: the polynomial, given some degree `\(d\)`,  
that minimizes any approximation error to the true function

---

# The Chebyshev basis

Chebyshev polynomials are defined by a recurrence relation,
`\begin{gather}
	T_0(x) = 1 \\
	T_1(x) = x  \\
	T_{n+1} = 2xT_n(x) - T_{n-1}(x)
\end{gather}`
and are defined on the domain `\([-1,1]\)`

--

In practice this is easy to expand to any interval `\([a,b]\)`

--

Chebyshev polynomials look similar to monomials but have better properties that are visually distinctive

--

Write two functions: `cheb_polys(n, x)` and `monomials(n, x)` that take in a degree of approximation `n`, and vector of points `x`,
and returns the values of a the basis function of degree `n` at all points `x`

--

Write a plotting function `plot_function(basis_function, x, n)` that takes an arbitrary basis function `basis_function` and plots all basis functions up to degree `n`

--

If you can't get the recurrence relation to work, you can use an alternative: `$$T_n(x) = cos(n \, arccos(x))$$`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "bash",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
